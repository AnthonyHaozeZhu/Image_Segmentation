{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import argparse\n",
    "import string\n",
    "import os.path as osp\n",
    "import operator\n",
    "# non-standard dependencies:\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_path='../data/visual_genome/'\n",
    "\"\"\"\n",
    "Our vocabulary will add __background__, COCO categories, <UNK>, PAD, BOS, EOS\n",
    "\"\"\"\n",
    "# remove bad words, and return final sentences (sent_id -> final)\n",
    "# count up the number of words\n",
    "#   count_thr = params['word_count_threshold']\n",
    "\n",
    "with open(osp.join(vg_path,'scene_graphs_pp.json'), 'r') as f:\n",
    "    images = json.load(f)\n",
    "print('sgpp loaded')\n",
    "with open(osp.join(vg_path, 'image_data_split1000.json'), 'r') as f:\n",
    "    imgs_info = json.load(f)\n",
    "    info_dict = {img['image_id']: img for img in imgs_info}\n",
    "\n",
    "Images = dict()\n",
    "for img in images:\n",
    "    info = info_dict[img['image_id']]\n",
    "    if info['split'] in ['train', 'val']:    \n",
    "        Images[img['image_id']] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "special = []\n",
    "def replace_special(name):\n",
    "    name = name.lower()\n",
    "    i = 0\n",
    "    while i < len(name):\n",
    "        c = name[i]\n",
    "        if (c > 'z' or c < 'a') and c != ' ':\n",
    "            if c not in special: \n",
    "                special.append(c)\n",
    "            name = name[:i] + ' ' + c + ' ' + name[i+1:]\n",
    "            i += 2\n",
    "#             print(name)\n",
    "        i += 1\n",
    "    return name\n",
    "\n",
    "\n",
    "def count_words_vg(source='names', Images = Images):\n",
    "    word2count = {}\n",
    "    for id, img in Images.items():\n",
    "        if source in ['names', 'attributes']:\n",
    "            for obj in img['objects']:\n",
    "                if source in obj:\n",
    "                    for string in obj[source]:\n",
    "                        string = replace_special(string)\n",
    "                        for wd in string.split():\n",
    "                            word2count[wd] = word2count.get(wd, 0) + 1\n",
    "        elif source == 'relationships':\n",
    "            for rel in img['relationships']:\n",
    "                string = rel['predicate']\n",
    "                string = replace_special(string)\n",
    "                for wd in string.split():\n",
    "                    word2count[wd] = word2count.get(wd, 0) + 1\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "    return word2count\n",
    "\n",
    "def merge_count(count1, count2):\n",
    "    if len(count1) < len(count2):\n",
    "        count3 = count1\n",
    "        count1 = count2\n",
    "        count2 = count3\n",
    "    count = dict(count1)\n",
    "    for w,c in count2.items():\n",
    "        count[w] = count.get(w, 0) + count2[w]\n",
    "    return count\n",
    "\n",
    "def add_count(count1, count2):\n",
    "    for w, c in count2.items():\n",
    "        count1[w] = count1.get(w, 0) + count2[w]\n",
    "    return count1\n",
    "\n",
    "def build_vocab(word2count, count_thr):\n",
    "    total_count = sum(word2count.values())\n",
    "    bad_words = [wd for wd, n in word2count.items() if n <= count_thr]\n",
    "    bad_count = sum([word2count[wd] for wd in bad_words])\n",
    "    \n",
    "    good_words_freq= [i for i in word2count.items() if i[1] > count_thr]\n",
    "    good_words_freq = sorted(good_words_freq, key=lambda x: -x[1])\n",
    "    good_words = [x[0] for x in good_words_freq]\n",
    "    good_freq = [x[1] for x in good_words_freq]\n",
    "    print(good_words_freq[:10])\n",
    "    print(good_words[:10])\n",
    "    print(good_freq[:10])\n",
    "    print('number of good words: %d' % len(good_words))\n",
    "    print('number of bad words: %d/%d = %.2f%%' \n",
    "          % (len(bad_words), len(word2count), len(bad_words)*100.0/len(word2count)))\n",
    "    print('number of UNKs in sentences: %d/%d = %.2f%%' \n",
    "          % (bad_count, total_count, bad_count*100.0/total_count))\n",
    "\n",
    "    # add UNK, BOS, EOS, PAD\n",
    "    vocab = ['<PAD>', '<UNK>', '<BOS>', '<EOS>'] + good_words\n",
    "    freq = [good_freq[0] * 10] * 4 + good_freq\n",
    "    return vocab, freq\n",
    "\n",
    "def build_lookup(included, count_thresh, vg_count=None, rc_count=None, fast_text_path='../data/fast_text/'):\n",
    "    lookup = {}\n",
    "    lookup['included'] = included\n",
    "    count = {}\n",
    "    if 'vg_names' in included:\n",
    "        count = add_count(count, vg_count['names'])\n",
    "    if 'vg_attributes' in included:\n",
    "        count = add_count(count, vg_count['attributes'])\n",
    "    if 'vg_relationships' in included:\n",
    "        count = add_count(count, vg_count['relationships'])\n",
    "        \n",
    "    if 'refcoco' in included:\n",
    "        count = add_count(count, rc_count['refcoco'])\n",
    "    if 'refcoco+' in included:\n",
    "        count = add_count(count, rc_count['refcoco+'])\n",
    "    if 'refcocog' in included:\n",
    "        count = add_count(count, rc_count['refcocog'])\n",
    "    \n",
    "    vocab, freq = build_vocab(count, count_thresh)\n",
    "    lookup['ix_to_word'] = vocab\n",
    "    lookup['freq'] = freq\n",
    "    ft_vocab = np.load(osp.join(fast_text_path, 'vocabulary_ft.npy'))\n",
    "    ft_vocab = list(ft_vocab)\n",
    "    ft_embeddings = np.load(osp.join(fast_text_path, 'embeddings_ft.npy'))\n",
    "    \n",
    "    embeddings = np.empty((len(vocab), 300))\n",
    "    for i, w in enumerate(vocab):\n",
    "        if w in ft_vocab:\n",
    "            ft_i = ft_vocab.index(w)\n",
    "            embeddings[i] = ft_embeddings[ft_i]\n",
    "        else:\n",
    "            embeddings[i] = np.random.randn(300) / 300.0\n",
    "            print(i, w, freq[i])\n",
    "    lookup['embeddings'] = embeddings\n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_name_count = count_words_vg('names')  # full vg: len 26160  sgpp 25972\n",
    "vg_att_count = count_words_vg('attributes')  # full vg: len 20284 sgpp 20196\n",
    "vg_rel_count = count_words_vg('relationships')  # full vg: len 7973 sgpp 7940"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(vg_name_count))\n",
    "print(len(vg_att_count))\n",
    "print(len(vg_rel_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_count(word2count):\n",
    "    count = word2count.values()\n",
    "    count.sort()\n",
    "    plt.plot(count)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plot_count(vg_name_count)\n",
    "plot_count(vg_att_count)\n",
    "plot_count(vg_rel_count)\n",
    "\n",
    "print(len(vg_name_count), len(vg_att_count), len(vg_rel_count))\n",
    "\n",
    "name_att_count = merge_count(vg_name_count, vg_att_count)\n",
    "print(len(name_att_count))\n",
    "vg_count = merge_count(name_att_count, vg_rel_count)\n",
    "print(len(vg_count))\n",
    "\n",
    "plot_count(name_att_count)\n",
    "plot_count(vg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh = 10\n",
    "print('name')\n",
    "name_vocab, _ = build_vocab(vg_name_count, thresh)\n",
    "print('\\natt')\n",
    "att_vocab, _ = build_vocab(vg_att_count, thresh)\n",
    "print('\\nrel')\n",
    "rel_vocab, _ = build_vocab(vg_rel_count, thresh)\n",
    "print('\\nname_att')\n",
    "name_att_vocab = build_vocab(name_att_count, thresh)\n",
    "print('\\nvg')\n",
    "vg_vocab, _ = build_vocab(vg_count, thresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vg_counts = {'names': vg_name_count,\n",
    "            'attributes': vg_att_count,\n",
    "            'relationships': vg_rel_count}\n",
    "\n",
    "vg_lookup = build_lookup(['vg_names', 'vg_attributes', 'vg_relationships'], 10, vg_counts)\n",
    "\n",
    "# np.save('../data/fast_text/wordcounts_vgpp_trainval.npy', vg_counts)\n",
    "np.save('../data/fast_text/lookup_vgpp_trainval.npy', vg_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "refer_dir = osp.join('..', 'pyutils', 'refer')\n",
    "sys.path.insert(0, refer_dir)\n",
    "from refer import REFER\n",
    "\n",
    "def count_words_refcocox(data_root = '../data', dataset='refcoco', splitBy='unc'):\n",
    "    refer = REFER(data_root, dataset, splitBy)\n",
    "    sentToTokens = refer.sentToTokens\n",
    "    # count the number of words\n",
    "    word2count = {}\n",
    "    for sent_id, tokens in sentToTokens.items():\n",
    "        for string in tokens:\n",
    "            string = replace_special(string)\n",
    "            for wd in string.split():\n",
    "                word2count[wd] = word2count.get(wd, 0) + 1\n",
    "    \n",
    "    # add category words\n",
    "    category_names = refer.Cats.values() + ['__background__']\n",
    "    for cat_name in category_names:\n",
    "        for wd in cat_name.split():\n",
    "                word2count[wd] = 1e5\n",
    "    return word2count\n",
    "\n",
    "rc_count = count_words_refcocox(dataset='refcoco')\n",
    "rcp_count = count_words_refcocox(dataset='refcoco+')\n",
    "rcg_count = count_words_refcocox(dataset='refcocog', splitBy='google')\n",
    "\n",
    "print(len(rc_count), len(rcp_count), len(rcg_count))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "plot_count(rc_count)\n",
    "plot_count(rcp_count)\n",
    "plot_count(rcg_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresh = 10\n",
    "print('rc')\n",
    "rc_vocab, _ = build_vocab(rc_count, thresh)\n",
    "print('\\nrc+')\n",
    "rcp_vocab, _ = build_vocab(rcp_count, thresh)\n",
    "print('\\nrcg')\n",
    "rcg_vocab, _ = build_vocab(rcg_count, thresh)\n",
    "print('\\nvg_rc')\n",
    "vg_rc_count = merge_count(vg_count, rc_count)\n",
    "vg_rc_vocab, _ = build_vocab(vg_rc_count, thresh)\n",
    "print('\\nvg_rc+')\n",
    "vg_rcp_count = merge_count(vg_count, rcp_count)\n",
    "vg_rcp_vocab, _ = build_vocab(vg_rcp_count, thresh)\n",
    "print('\\nvg_rcg')\n",
    "vg_rcg_count = merge_count(vg_count, rcg_count)\n",
    "vg_rcg_vocab, _ = build_vocab(vg_rcg_count, thresh)\n",
    "print('\\nvg_rc_rc+')\n",
    "vg_rc_rcp_count = merge_count(vg_rc_count, rcp_count)\n",
    "vg_rc_rcp_vocab, _ = build_vocab(vg_rc_rcp_count, thresh)\n",
    "print('\\nvg_rcall')\n",
    "vg_rcall_count = merge_count(vg_rc_rcp_count, rcg_count)\n",
    "vg_rcall_vocab, _ = build_vocab(vg_rcall_count, thresh)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'rca' means ['refcoco', 'refcoco+', 'refcocog']\n",
    "rca_counts = {'refcoco': rc_count,\n",
    "              'refcoco+': rcp_count,\n",
    "              'refcocog': rcg_count}\n",
    "\n",
    "rca_lookup = build_lookup(['refcoco', 'refcoco+', 'refcocog'], 10, rc_count=rca_counts)\n",
    "\n",
    "# np.save('../data/fast_text/wordcounts_rca.npy', rca_counts)\n",
    "np.save('../data/fast_text/lookup_rca.npy', rca_lookup)\n",
    "\n",
    "vg_rca_lookup = build_lookup(['vg_names', 'vg_attributes', 'vg_relationships', 'refcoco', 'refcoco+', 'refcocog'],\n",
    "                             10, vg_counts, rca_counts)\n",
    "\n",
    "np.save('../data/fast_text/lookup_vgpp_tv_rca.npy', vg_rca_lookup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lookup = np.load('../data/fast_text/lookup_vgpp_tv_rca.npy')\n",
    "lookup.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refer_dir = osp.join('..', 'pyutils', 'refer')\n",
    "sys.path.insert(0, refer_dir)\n",
    "from refer import REFER\n",
    "\n",
    "refer = REFER(data_root='../data', dataset='refcoco', splitBy='unc')\n",
    "category_names = refer.Cats.values()\n",
    "print(category_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/visual_genome/name_att_rel_count_pp.json', 'r') as f:\n",
    "    count_info = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = sorted(count_info['name'].items(), key=lambda x: -x[1])\n",
    "print(c[:100])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([i[1] for i in c[:500]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
