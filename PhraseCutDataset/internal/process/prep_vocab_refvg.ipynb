{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os.path as osp\n",
    "import json\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading refer_filtered_instance_miniv.json\n",
      "loading refer_filtered_instance_val.json\n",
      "loading refer_filtered_instance_test.json\n",
      "loading refer_filtered_instance_train.json\n"
     ]
    }
   ],
   "source": [
    "splits = ['miniv', 'val', 'test', 'train']\n",
    "\n",
    "ref_tasks = []\n",
    "for s in splits:\n",
    "    print('loading refer_filtered_instance_%s.json' % s)\n",
    "    with open('/mnt/nfs/work1/elm/chenyun/PhraseCutPano/data/refvg/amt_result/refer_filtered_instance_%s.json' % s, 'r') as f:\n",
    "        ref_tasks += json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'phrase_structure': {'attributes': ['seated'], 'type': 'name', 'name': 'hill', 'relations': []}, 'task_id': '2328797__3342390', 'iou': 0.10582814686158151, 'instance_boxes': [[50.505, 259.185, 150.295, 181.375]], 'iop': 1.0, 'image_id': 2328797, 'ann_ids': [3342390], 'Polygons': [[[[67.71000000000001, 305.805], [58.83, 283.605], [57.72, 275.28], [53.834999999999994, 259.185], [50.505, 264.18], [53.28, 283.05], [52.724999999999994, 288.045], [56.055, 314.685], [76.59, 387.945], [78.81, 439.005], [89.91000000000001, 439.56], [192.03, 439.56], [193.14, 346.32], [199.245, 342.435], [199.79999999999998, 338.54999999999995], [79.92, 339.66], [62.714999999999996, 285.825]]]], 'phrase': 'seated hill', 'turk_id': 'A2P7Z2TKGACY2E', 'iob': 0.10582814686158151}\n"
     ]
    }
   ],
   "source": [
    "print(ref_tasks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fast_text_path = '/mnt/nfs/work1/elm/chenyun/PhraseCutPano/data/fast_text/'\n",
    "ft_vocab = np.load(osp.join(fast_text_path, 'vocabulary_ft.npy'))\n",
    "ft_vocab = list(ft_vocab)\n",
    "ft_embeddings = np.load(osp.join(fast_text_path, 'embeddings_ft.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_count = dict()\n",
    "special = []\n",
    "\n",
    "\n",
    "def replace_special(name):\n",
    "    name = name.lower()\n",
    "    i = 0\n",
    "    while i < len(name):\n",
    "        c = name[i]\n",
    "        if (c > 'z' or c < 'a') and (c < '0' or c > '9') and c != ' ':\n",
    "            if c not in special: \n",
    "                special.append(c)\n",
    "            name = name[:i] + ' ' + c + ' ' + name[i+1:]\n",
    "            i += 2\n",
    "        i += 1\n",
    "    return name\n",
    "\n",
    "\n",
    "for task in ref_tasks:\n",
    "    phrase = task['phrase']\n",
    "    phrase = replace_special(phrase)\n",
    "    words = phrase.split()\n",
    "    for w in words:\n",
    "        word_count[w] = word_count.get(w, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7810\n",
      "['-', \"'\", ',', ':', '<', '.', '/', '?', '*', '\"', '\\\\', '&', '\\x00', '`', '!', ']', '[', '+', '@', '(', ')']\n",
      "1786\n"
     ]
    }
   ],
   "source": [
    "print(len(word_count))\n",
    "print(special)\n",
    "ws = [w for w, c in word_count.items() if c > 20]\n",
    "print(len(ws))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<PAD>', '<UNK>', '<BOS>', '<EOS>', 'white', 'on', 'black', 'blue', 'green', 'man']\n",
      "[1000, 1000, 1000, 1000, 25410, 18910, 14959, 11968, 11366, 10976]\n"
     ]
    }
   ],
   "source": [
    "wc_sorted = [(w, c) for w, c in sorted(word_count.items(), key=lambda kv: -kv[1])]\n",
    "\n",
    "vocab = ['<PAD>', '<UNK>', '<BOS>', '<EOS>'] + [wc[0] for wc in wc_sorted]\n",
    "freq = [1000] * 4 + [wc[1] for wc in wc_sorted]\n",
    "\n",
    "print(vocab[:10])\n",
    "print(freq[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "lookup = dict()\n",
    "lookup['included'] = ['refvg_train', 'refvg_val', 'refvg_test', 'refvg_miniv']\n",
    "lookup['ix_to_word'] = vocab\n",
    "lookup['freq'] = freq\n",
    "\n",
    "embeddings = np.empty((len(vocab), 300))\n",
    "for i, w in enumerate(vocab):\n",
    "    if w in ft_vocab:\n",
    "        ft_i = ft_vocab.index(w)\n",
    "        embeddings[i] = ft_embeddings[ft_i]\n",
    "    else:\n",
    "        embeddings[i] = np.random.randn(300) / 300.0\n",
    "        print(i, w, freq[i])\n",
    "lookup['embeddings'] = embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('/mnt/nfs/work1/elm/chenyun/PhraseCutPano/data/fast_text/lookup_refvg_all.npy', lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
